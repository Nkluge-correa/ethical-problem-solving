# What is Truthfulness? ðŸ”Ž

_6-minute read._

The idea of truthfulness is a fundamental ethical principle in AI research and development, aiming to prevent the spread of false information from AI systems. Truthfulness is especially crucial in generative AI, where algorithms can produce content resembling human-generated media with such proficiency that, currently, distinguishing them is becoming a task that requires augmentation. Ultimately, following principles of truthfulness protects the integrity of information and helps counter the harmful effects of AI-generated misinformation in society.

According to [Worldwide AI Ethics](https://en.airespucrs.org/worldwide-ai-ethics), truthfulness can be defined as:

> "_...the idea that AI technologies must provide truthful information. It is also related to the idea that people should not be deceived when interacting with AI systems. This principle is strongly related to the mitigation of automated means of disinformation._"

For the EPS, generative AI is the only type of application and technology with which the principle of truthfulness is concerned. Meanwhile, the treatment of errors in machine learning models falls more aptly under the reliability principle, which pertains to the robustness and predictability of AI systems. However, in generative AI, where algorithms produce content that mimics human creativity and expression, the imperative also comes with a need for "_grounding outputs to factual reality_." In this context, ensuring truthfulness is of paramount importance, as the potential for AI-generated content to deviate from factual accuracy poses considerable ethical and societal implications depending on how the system is applied.

The epistemological inquiry into the nature of truth is a philosophical pursuit beyond the EPS's scope. While acknowledging the complexity inherent in grappling with the broader concept of truth, this work adopts a pragmatic stance, focusing instead on operational definitions of untruthfulness or falsehoods within the context of AI-generated content ([source](https://arxiv.org/abs/2110.06674)). Herein, we define falsehoods as outputs generated by AI systems that, given their training data, should have been recognized as likely to be false. This operationalization grounds the notion of truth within generative AI systems by referencing the scope of their training data. For instance, if a model trained exclusively on Shakespearean novels were to attempt to respond to a query regarding Brazil's capital, its failure would not be categorized as untrue but mere model incompetence. Conversely, a model trained on factual datasets such as Wikipedia, yet producing erroneous information, would be deemed to have generated a falsehood, as it deviates from the solid grounding of its training corpus.

Falsehoods can also emerge from the deliberate or unintentional misrepresentation of human identity or agency. This encompasses cases where AI models, either by design or through hallucinatory processes, generate content intended to impersonate natural human agents. For instance, if an AI chatbot, trained on a dataset of human conversations, fabricates dialogue purporting to originate from a specific individual, it engenders deception by creating a false impression of human interaction ([source](https://www.vice.com/en/article/jg5ew4/gpt4-hired-unwitting-taskrabbit-worker)). Similarly, instances of deepfake technology, where AI systems produce counterfeit audio or video content resembling real individuals, exemplify the implications of falsehoods arising from impersonation. Thus, in addition to deviations from factual accuracy, the characterization of falsehoods within generative AI encompasses instances where AI systems transgress boundaries of human identity and authenticity.

To aid developers in reducing the impacts caused by model falsehood, the EPS recommends the following measures differentially regarding the possible effects of an application:

- **Disclosure:** Non-disclosure of AI involvement can lead to a sense of deception, especially if users are unaware that they are interacting with an AI system.

- **Retrieval Augmented Generation:** It improves the quality of outputs by grounding the model on external sources of knowledge to supplement the internal representation of information.

- **Watermarking:** It involves tracing the origin or ownership of the AI system, its components, or outputs. It can help protect against unauthorized modifications or tampering with AI outputs.

- **Moderation:** It involves automated processes and human oversight to ensure the generated content meets the desired standards.

## Why Should You Care? ðŸ¤”

Generative models have pushed the boundaries of what we can distinctively say is human-made. With that, the risk of false or misleading information from AI-produced content is ever-present. AI-generated images, texts, and videos have already created fake news, propaganda, false identities, and disinformation. Under this scenario, the principle of truthfulness has never been so current.

Below, we can find some examples of AI falsehoods in the wild.

### Lying AI Assistants

Today's AI chatbots can generate plausible solutions to practically any inquiry. However, without the correct guardrails in place, given the way many of these models are developed, the "produce something" may become a more pressing matter than "produce something factual," prompting models to generate anything that "could plausibly be."

Countless examples of these tools fail to answer basic factual inquiries or invent falsehoods, complete with realistic details and fabricated citations, further warning us about their potential to mislead users. Two common instances that characterize how untruthful these chatbots can be are mimicking popular human misconceptions, e.g., "Can cough effectively stop a heart attack?" ([source](https://arxiv.org/abs/2109.07958)) and the fabrication of convincing information which in the end is entirely untied to reality e.g., when ChatGPT made up court citations ([source](https://www.theguardian.com/technology/2023/jun/23/two-us-lawyers-fined-submitting-fake-court-citations-chatgpt)).

In light of the remarkable capabilities demonstrated by contemporary AI chatbots, it becomes increasingly imperative to implement stringent safeguards to ensure the integrity of their outputs. Failure to establish adequate guardrails may engender a situation where the imperative to "produce something" supersedes the necessity to generate factually accurate content. As such, addressing the issue of untruthfulness within AI chatbots demands a concerted effort to establish robust mechanisms that prioritize producing factually sound responses.

### Deceptive Image Generation

Truthfulness plays a critical role in image generation systems. Advanced text-to-image tools can edit existing photographs seamlessly, generate images using particular styles, and even produce pieces in almost any artistic style. Currently, systems like DALLÂ·E 3, Midjourney, Lensa, and Stable Diffusion sparked unease and controversy among creatives, from marketing professionals to artists.

Concerning truthfulness, at the moment, many systems are available to anyone wishing to produce photo-realistic images using nothing more than text prompts. For example, the photo series of Pope Francis in a puffer jacket ([source](https://www.cbsnews.com/news/pope-francis-puffer-jacket-fake-photos-deepfake-power-peril-of-ai/)) is a prime example of the potential risks and implications of these image generation systems, given that when such images were released, a significant audience unfamiliar with the concept of GenAI was caught by surprise.

Ensuring that the generated outputs of such systems are not tailored to spread misinformation is crucial. Currently, the Coalition for Content Provenance and Authenticity (C2PA), a joint project of several tech companies, seeks to address the critical issue that is the provenance of digital media, given the seriousness that fake media sharing may scale up in the future.

## Final Remarks

The exploration of truthfulness within AI ethics underscores the critical imperative to ensure the integrity and reliability of AI-generated content. While acknowledging the complexities surrounding the epistemological nature of truth, the pragmatic approach adopted herein underscores the practical significance of establishing robust mechanisms to mitigate the proliferation of misinformation and deceptive content. As AI technologies advance and permeate various facets of society, truthfulness will remain paramount in safeguarding against the potential harms posed by untruthful AI-generated content.

## Extra reading

- Evans, O., Cotton-Barratt, O., Finnveden, L., Bales, A., Balwit, A., Wills, P., Righetti, L. and Saunder, W. [Truthful AI: Developing and governing AI that does not lie](https://arxiv.org/abs/2110.06674).  _ArXiv_, 2021.
- Lin, S., Hilton, J., and Evans, O. [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958).  _ArXiv_, 2022.
- Vaccari, C., & Chadwick, A. [Deepfakes and disinformation: Exploring the impact of synthetic political video on deception, uncertainty, and trust in news](https://journals.sagepub.com/doi/full/10.1177/2056305120903408). _Social Media+ Society_, 6(1), 2020.
- Kadavath, Saurav, et al. [Language models (mostly) know what they know](https://arxiv.org/abs/2207.05221).  _ArXiv_, 2022.
- [Coalition for Content Provenance and Authenticity - C2PA](https://c2pa.org/). _Online_.
