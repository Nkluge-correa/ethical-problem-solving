<h1>Intermediate</h1>
<p>EPS uses an evaluation matrix to calculate any potential impacts connected with an application. The three impact categories in this matrix are <strong>Low</strong>, <strong>Intermediate</strong>, and <strong>High</strong>. From low to high, each category has a set of suggestions.</p>
<p>EPS measures the level of impact of an application regarding sustainability through how likely they are to cause environmental harm. We quantify environmental harm in the EPS with two metrics: <strong>energy consumption</strong> (kWh) and <strong>estimated carbon emissions</strong> (KgCO2eq). KgCO2eq is a standardized measure used to express the global warming potential of various greenhouse gases. An application with an <strong>intermediate level of impact</strong> is likely to cause harm, i.e., has an energy consumption and estimated carbon emissions higher than 0.1 kWh and 0.05 KgCO2eq and lower than 1,500 kWh and 500 KgCO2eq during training (equivalent to driving a car for ~ 2,000 km).</p>
<p>Here are some examples of applications classified as <strong>intermediate impact</strong> according to the EPS:</p>
<ul>
<li><strong>Deep neural networks</strong> with less than 2 billion parameters trained up to the <a href="https://www.cerebras.net/model-lab/" target="_blank">optimal data scaling point</a> (<a href="https://arxiv.org/abs/2203.15556" target="_blank">source</a>).</li>
</ul>
<blockquote>
<p><strong>Note: Small models (&lt; 2B) can also pass this upper threshold if trained for extended periods (e.g., <a href="https://github.com/jzhang38/TinyLlama" target="_blank">TinyLlama</a>).</strong></p>
</blockquote>
<p>The following recommendations are related to the principle of <strong>sustainability</strong> in applications with an <strong>Intermediate</strong> level of impact.</p>
<h2>Recommendations</h2>
<p>An application with an <strong>Intermediate level of impact</strong> should implement the following measures:</p>
<ul>
<li>
<p><strong>Carbon Tracking:</strong> Carbon tracking allows organizations to quantify their environmental impact for accountability and monitoring.</p>
</li>
<li>
<p><strong>Use of open-source initiatives:</strong> Participating in collaborative and open-source initiatives helps the community minimize the amount of resources required to create and re-create the same types of technologies.</p>
</li>
<li>
<p><strong>Energy-efficient models and hardware:</strong> Designing AI models that prioritize energy efficiency while selecting energy-efficient hardware components for training and inference can reduce AI systems' energy consumption and resource requirements.</p>
</li>
</ul>
<h2>How to increase sustainability?</h2>
<h3>Carbon Tracking</h3>
<p><strong>Carbon tracking</strong> is an essential practice in managing the environmental impacts of AI systems. Organizations can comprehensively understand their carbon footprint by calculating and monitoring quantities like energy consumption and estimated carbon emissions. This information is a foundation for driving sustainable practices, optimizing energy efficiency, reducing greenhouse gas emissions, and investing in offsets. But how can one measure their carbon footprint?</p>
<p>Carbon footprint is an index that makes it possible to compare the total amount of greenhouse gases an activity adds to the atmosphere, i.e., how much carbon dioxide equivalents (CO2eq) have been produced. CO2eq is the product of two main factors (<a href="https://arxiv.org/abs/1911.08354" target="_blank">source</a>): <strong>C</strong> and <strong>E:</strong></p>
<ul>
<li>
<p><strong>C</strong> = the carbon intensity of the electricity consumed (grams of COâ‚‚ emitted per kilowatt-hour of electricity).</p>
</li>
<li>
<p><strong>E</strong> = the energy consumed by the computational process (kilowatt-hour).</p>
</li>
</ul>
<p>The carbon intensity is the weighted average of the emissions from the different energy sources tied to the energy grid being utilized. This measure changes depending on your location, while the world average emission by kWh is 475 gCO2.eq (<a href="https://www.iea.org/reports/global-energy-co2-status-report-2019/emissions" target="_blank">source</a>). Hence, if you know the carbon intensity of your energy grid and the amount of energy used, you can estimate your carbon emissions and footprint.</p>
<p>Luckily, you do not have to calculate these values by hand since much of it is already hand-coded in tools designed to perform the type of tracking we are recommending. For example, <a href="https://github.com/mlco2/codecarbon" target="_blank">CodeCarbon</a> is a Python library for estimating and tracking carbon emissions related to computational processes. The following tutorial teaches how to use the main functionalities of this library [<a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Accountability/CO2-Emission-tracking/emission_tracker.ipynb" target="_blank">ðŸ‘‰notebook</a>]. To learn more about what CodeCarbon can offer (e.g., visualization panels for emission reports), read their <a href="https://mlco2.github.io/codecarbon/" target="_blank">documentation</a>, and see how carbon tracking can be implemented as a part of tools aimed at helping developers choose the most carbon-efficient algorithm with the <a href="https://tw-yoo.github.io/miev/#/" target="_blank">Model Inference Emission Visualizer</a>.</p>
<p>Besides CodeCarbon, here are some other tools that you can use to estimate your carbon emissions:</p>
<ul>
<li>
<p><a href="https://mlco2.github.io/impact/" target="_blank">Machine Learning Emissions Calculator</a> is a tool to estimate the carbon emissions based on your energy consumption and geo-location/cloud provider, training ML models produce.</p>
</li>
<li>
<p><a href="https://github.com/sb-ai-lab/Eco2AI" target="_blank">Eco2AI</a> and <a href="https://github.com/lfwa/carbontracker?tab=readme-ov-file#carbontracker" target="_blank">Carbontracker</a> are two other Python libraries for emission tracking during the training of deep learning models.</p>
</li>
<li>
<p><a href="https://github.com/datarootsio/mlflow-emissions-sdk" target="_blank">MLflow-emissions-sdk</a> is a log management tool that helps you integrate carbon tracking into your ML workflow (it uses CodeCarbon under the hood).</p>
</li>
</ul>
<h3>Use of open-source initiatives</h3>
<p>Open-source initiatives provide ways for sharing knowledge, research, and their byproducts, allowing developers to build upon the work of others and preventing organizations from repeatedly reinventing the same piece of software. By openly sharing code, models, datasets, and resources, these initiatives enable collective innovation and iteration, leading to a more sustainable AI ecosystem. If you would like to, for example, train a computer vision model for object detection, instead of training your CNN from scratch, you can download the pre-trained weights from something like Resnet and fine-tune it by a fraction of the resources required to train it.</p>
<p>Many platforms make models and datasets available to developers, while major deep learning frameworks already come with models and datasets integrated into their ecosystem. Below is a non-exhaustive list of open-source resources that can help you develop AI solutions without reinventing the wheel:</p>
<ul>
<li>
<p><strong>Torchvision</strong> contains definitions of several models and their pre-trained weights for different tasks, including image classification, pixel-wise semantic segmentation, object detection, instance segmentation, person keypoint detection, video classification, and optical flow (<a href="https://pytorch.org/vision/stable/models.html#models-and-pre-trained-weights" target="_blank">source</a>).</p>
</li>
<li>
<p><strong>PyTorch Hub</strong> contains a great variety of models for several different applications (language, vision, audio, generative, etc.) (<a href="https://pytorch.org/hub/" target="_blank">source</a>).</p>
</li>
<li>
<p><strong>TensorFlow</strong> also possesses a large list of models and datasets as part of its open-source platform for ML (<a href="https://www.tensorflow.org/resources/models-datasets" target="_blank">source</a>) (<a href="https://github.com/tensorflow/models/tree/master/official#tensorflow-official-models" target="_blank">source</a>) (<a href="https://www.tensorflow.org/datasets" target="_blank">source</a>).</p>
</li>
<li>
<p><strong>Kaggle</strong> also has hundreds of trained, ready-to-deploy machine learning models, besides a great number of datasets hosted in their platform (<a href="https://www.kaggle.com/models" target="_blank">source</a>).</p>
</li>
<li>
<p><strong>Hugging Face</strong> provides an excellent UI for selecting already-trained models for dozens of tasks (<a href="https://huggingface.co/tasks" target="_blank">source</a>).</p>
</li>
<li>
<p><strong>Timm</strong> is a library tied to the Hugging Face ecosystem that contains several SOTA computer vision models and utilities (<a href="https://github.com/pprp/timm?tab=readme-ov-file#pytorch-image-models" target="_blank">source</a>).</p>
</li>
<li>
<p>This <strong>repository</strong> lists several open-source LLMs and their respective licenses (<a href="https://github.com/eugeneyan/open-llms?tab=readme-ov-file#open-llms" target="_blank">source</a>).</p>
</li>
</ul>
<h3>Energy-efficient models and hardware</h3>
<p>The development stage of a particular system can be a critical moment in attaining sustainability, given that the model design considerably influences the energy consumption of the whole system. Specific design and architectural choices significantly impact the resources needed for training and running inference on large models.</p>
<p>There are three main components we have to consider before training a large model if we wish to optimize its resource consumption: model <strong>architecture</strong>, <strong>dataset size</strong>, and the <strong>hardware</strong> the models are going to be trained:</p>
<p>In terms of <strong>architecture</strong>, when creating a large neural network, we aspire to use the best and most efficient components, given that these can shorten the required training, i.e., faster training runs -&gt; less computation -&gt; reduced costs. Here are some examples of how you can achieve this:</p>
<ul>
<li>
<p>In the case of computer vision, certain types of basic operations can be optimized to require less computations. For example, when creating a CNN, instead of using the standard convolutional layer, <a href="https://arxiv.org/abs/1610.02357" target="_blank">depthwise separable convolutions</a>, which are a type of layer implemented on standard DL libraries (<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D" target="_blank">TensorFlow</a>, <a href="https://github.com/seungjunlee96/Depthwise-Separable-Convolution_Pytorch" target="_blank">PyTorch</a>), require less computation, resulting in faster training and inference, with almost no loss in performance. You can learn more about the substitution of <strong>Conv2D</strong> layers by <strong>SeparableConv2D</strong> in this tutorial [<a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Accountability/CO2-Emission-tracking/carbon_emission_cv.ipynb" target="_blank">ðŸ‘‰notebook</a>].</p>
</li>
<li>
<p>In the case of language models, the current most significant bottleneck in computational resources is the <a href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html" target="_blank">attention mechanism</a>, which has a quadratic computational complexity in sequence length. To alleviate this bottleneck, we have a few options. The first is to abandon attention and rely on attention-free models, like parallelizable RNNs (<a href="https://www.rwkv.com/" target="_blank">RWKV</a>) or state space models like Mamba (<a href="https://github.com/state-spaces/mamba?tab=readme-ov-file#mamba" target="_blank">source</a>). Currently, RWKV is already implemented in the Transformers library (<a href="https://huggingface.co/docs/transformers/model_doc/rwkv" target="_blank">source</a>). The other option is to utilize a more efficient implementation of the attention mechanism, like <a href="https://github.com/Dao-AILab/flash-attention" target="_blank">FlashAttention</a>, which can speed up inference and training considerably. FlashAttention is already implemented in libraries like <a href="https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2" target="_blank">Transformers</a>, <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" target="_blank">PyTorch</a>, and <a href="https://huggingface.co/docs/transformers/perf_infer_gpu_one#-optimum" target="_blank">Optimum</a>. Besides, one can also implement, instead of the vanilla attention mechanism, more efficient ways to compute attention (<a href="https://github.com/kyegomez/SparseAttention" target="_blank">Sparse Attention</a>). Many modern models, like Llama 2 and Mistral 7B, utilize attention mechanisms that allow for faster computation, like <a href="https://paperswithcode.com/method/grouped-query-attention" target="_blank">Grouped Query Attention</a> and <a href="https://paperswithcode.com/method/sliding-window-attention" target="_blank">Sliding Window Attention</a>. Lastly, one can also develop and use sparse models, like a Sparse Mixture of Experts (<a href="https://huggingface.co/blog/moe" target="_blank">Sparse MoEs</a>), which use far fewer resources, allowing for the training and use of much larger models for a fraction of the computational cost. <a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1" target="_blank">Open-source versions of Sparse MoEs</a> are already available to the public under permissive licenses.</p>
</li>
</ul>
<p>Regarding <strong>dataset</strong>, we can use scaling laws to estimate our dataset's optimal size relative to our model's size. For example, according to the <a href="https://arxiv.org/abs/2203.15556" target="_blank">Chinchilla</a> scaling laws, we can model language modeling loss as a function of model size(the number of parameters) and training dataset size (the number of tokens). Generally, these laws point to a golden ratio of 20 tokens per parameter for a given language model, also saying that as we increase our model size, we should increase our dataset size proportionally. Hence, provided that you know the model size you wish to train, you can design your dataset according to this optimal range:</p>
<ul>
<li>
<p>This <a href="https://www.cerebras.net/model-lab/" target="_blank">tool</a> can give you training and inference estimations for the resources required to train large models.</p>
</li>
<li>
<p>If you are interested in training optimally sized LLMs, we recommend this <a href="https://github.com/Nkluge-correa/TeenyTinyLlama?tab=readme-ov-file#teenytinyllama-open-source-tiny-language-models-trained-in-brazilian-portuguese" target="_blank">TeenyTinyLlama</a> repository.</p>
</li>
<li>
<p>To learn about the laws that govern the scaling of computer vision models, we recommend the following study (<a href="https://arxiv.org/abs/2106.04560" target="_blank">source</a>).</p>
</li>
</ul>
<p>Regarding <strong>hardware</strong>, hardware selection is crucial in developing and deploying AI systems, directly impacting their efficiency, performance, and energy consumption. AI workloads often require extensive computational power, and selecting hardware components optimized for AI tasks can deliver superior performance while minimizing energy consumption.</p>
<p>As a first approach to optimizing your awareness, we recommend researching what are the most efficient:</p>
<ul>
<li>
<p>This <a href="https://www.tomshardware.com/features/graphics-card-power-consumption-tested" target="_blank">publication</a> illustrates a test made with more than 50 GPUs from different brands, showing the relationship between power consumption and efficiency.</p>
</li>
<li>
<p>This repository provides a simple <a href="https://github.com/MTDoven/GPU-Speed-Test" target="_blank">speed test</a> for GPUs. It also contains comparisons of the most commercially available GPUs on the market.</p>
</li>
<li>
<p>The <a href="https://mtli.github.io/gpubench/" target="_blank">Deep Learning GPU Benchmark</a> compares GPUs in terms of their latency regarding training, inference, and complexity of the task under consideration.</p>
</li>
</ul>
<p>This type of research will allow you to find the GPUs that can give you the most FLOPs per resource unit (kWh). Also, the hardware that usually produces better results is the most recent and expensive product in the market. State-of-the-art GPUs are generally quite expensive, but they can drastically speed up training runs and minimize costs in the long run. For example:</p>
<ul>
<li>
<p>Many optimization tricks are only available for the most recent graphic cards (e.g., ampere architecture, hopper architecture, etc.), like the use of <a href="https://github.com/Dao-AILab/flash-attention" target="_blank">FlashAttention2</a>.</p>
</li>
<li>
<p>Ampere GPUs have exclusive math modes, like <a href="https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices" target="_blank">TF32</a>, that allow for a 3X increase in throughput.</p>
</li>
<li>
<p>For more information on how to take the most out of your training runs, we recommend the following two articles (<a href="https://huggingface.co/docs/transformers/perf_train_gpu_one" target="_blank">source</a>, <a href="https://huggingface.co/docs/transformers/perf_train_gpu_many" target="_blank">source</a>). Further resources for model optimization can be found <a href="https://www.tensorflow.org/model_optimization" target="_blank">here</a> and <a href="https://developer.nvidia.com/tensorrt" target="_blank">here</a>.</p>
</li>
</ul>
<p>Something that can also boost the efficiency of your hardware is to tailor the hardware itself to your needs. For training or inference, <strong>field-programmable gate arrays</strong> (<strong>FPGAs</strong>) are customizable circuits for specific AI tasks. These are often used in edge computing and IoT applications, given that they can achieve high performance while being power-efficient. You can learn more about FPGAs and how and when to use them <a href="https://www.run.ai/guides/gpu-deep-learning/fpga-for-deep-learning" target="_blank">here</a>. In the same spirit, <strong>application-specific integrated circuits</strong> (<strong>ASICs</strong>) can also be used to make ML pipelines more efficient (<a href="https://research.ibm.com/blog/ibm-artificial-intelligence-unit-aiu" target="_blank">source</a>).</p>
<blockquote>
<p><strong>Note:</strong> If you are interested in learning how to optimize the inference of large neural networks on CPUs, we recommend you look into projects like <a href="https://github.com/ggerganov/llama.cpp" target="_blank">Llama.cpp</a>, <a href="https://github.com/ggerganov/whisper.cpp" target="_blank">Whisper.cpp</a>, and <a href="https://github.com/karpathy/llama2.c" target="_blank">Llama2.c</a>.</p>
</blockquote>
<p>The methods cited above can help you lower the resource consumption of your system, especially during training. However, a whole other family of methods can improve this efficiency even more, allowing you to fine-tune modes and serve them by a fraction of the original resources it would take. These are methods for <strong>efficient fine-tuning</strong> and <strong>model quantization</strong>.</p>
<p>Fine-tuning a pre-trained model to a specific task is common in modern deep learning, given that foundation models are the base for many applications and systems. This process, which involves updating the parameters of a neural network through additional gradient updates, can narrow down and maximize the capabilities of pre-trained models. However, full fine-tuning optimizes all parameters, which can be very resource-intensive, especially if your foundation model is large. To help you with this issue, you may want to employ <strong>Parameter Efficient Fine-Tuning</strong> (<strong>PEFT</strong>) techniques.</p>
<p><strong>PEFT</strong> encompasses a collection of methods aimed at fine-tuning large models in a computationally efficient manner, preserving performance without the resource-intensive nature of full fine-tuning. This process is done by adding extra adapters (extra matrices of weights) to the linear layers on your model and training only these adapters while keeping the rest of the network frozen. This process can significantly decrease the number of trainable parameters during a fine-tuning run while generating almost the same performance as full fine-tuning. Several techniques to perform <strong>PEFT</strong> are already implemented in the Transformers library, which possesses excellent documentation on how to do it (<a href="https://huggingface.co/docs/peft/index" target="_blank">source</a>). The main techniques we recommend in this section are <strong>Low-rank adaptation</strong> (<strong>LoRA</strong>), <strong>Quantization</strong>, and <strong>Quantized low-rank adaptation</strong> (<strong>QLoRA</strong>). These techniques are some of the most widely used, parameter-efficient finetuning techniques for training large neural networks, which, in essence, reduce the number of trainable parameters in a model or reduce the precision in which these parameters are expressed:</p>
<ul>
<li>
<p>With <strong>LoRA</strong>, instead of fine-tuning all the weights that constitute the weight matrix of the model, we only fine-tune two smaller matrices that approximate this larger matrix. These matrices form the LoRA adapter. The size of your adapter depends on how much of the network will have a corresponding adapter and what the rank of these adapters will be. As you expand the size and rank of your adapters, you approximate a full fine-tuning, i.e., LoRA is a generalization of full fine-tuning (<a href="https://www.youtube.com/watch?v=DhRoTONcyZE" target="_blank">source</a>). You can learn more about LoRA <a href="https://lightning.ai/pages/community/tutorial/lora-llm/" target="_blank">here</a>, and the following guides explain how to configure models for LoRA and fine-tune them (<a href="https://huggingface.co/docs/peft/developer_guides/lora#lora" target="_blank">source</a>, <a href="https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms" target="_blank">source</a>).</p>
</li>
<li>
<p><strong>Quantization</strong> reduces the precision of numerical values used to store the model's parameters. For example, instead of using full-precision (float32), we can load models at half-precision (float16) or even more quantized versions (like 8-bit or 4-bit integers) and still get the same level of performance. Most of the large models developed today are trained with <a href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#mixed-precision-training" target="_blank">half-precision</a> (flot16 or bfloat16). In short, the main advantage of quantizing a model is reducing memory footprint and latency, which can optimize your model's throughput and resource consumption. To perform quantization, we recommend methods like <a href="https://github.com/casper-hansen/AutoAWQ?tab=readme-ov-file#autoawq" target="_blank">AutoAWQ</a>, <a href="https://github.com/AutoGPTQ/AutoGPTQ?tab=readme-ov-file#autogptq" target="_blank">AutoGPTQ</a>, and <a href="https://github.com/TimDettmers/bitsandbytes" target="_blank">Bitsandbytes</a>. The following tutorials can help you get familiar with this process [<a href="https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY" target="_blank">ðŸ‘‰ notebook</a>, <a href="https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing" target="_blank">ðŸ‘‰ notebook</a>, <a href="https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing" target="_blank">ðŸ‘‰ notebook</a>]. Also, the Transformers library has integration for all of these quantization methods (<a href="https://huggingface.co/docs/transformers/v4.37.2/quantization" target="_blank">source</a>). CL frameworks like TensorFlow and PyTorch also provide quantization schemes and models for developers (<a href="https://www.tensorflow.org/tutorials/optimization/compression" target="_blank">source</a>, <a href="https://pytorch.org/docs/stable/quantization.html" target="_blank">source</a>).</p>
</li>
<li>
<p><strong>QLoRA</strong> is a combination of the last two approaches, i.e., quantize the model -&gt; perform a LoRA fine-tuning on its quantized version, which dramatically diminishes the memory footprint and resource consumption of very large models (<a href="https://github.com/artidoro/qlora" target="_blank">source</a>). The following tutorial shows how to fine-tune a 7B parameter model on consumer-grade GPU using QLoRA  [<a href="https://colab.research.google.com/github/brevdev/notebooks/blob/main/mistral-finetune.ipynb" target="_blank">ðŸ‘‰ notebook</a>].</p>
</li>
</ul>
<blockquote>
<p><strong>Note:</strong> Certain tasks are unsuitable for PEFT. For example, full fine-tuning is needed if you must train new embeddings in a language model or adapt a multilingual LLM to become monolingual. However, a PEFT with low-rank adaptations is sufficient for many tasks to produce results as good as full fine-tuning. In short, the more you need to move away from your foundation, the more you will need to adapt it.</p>
</blockquote>
